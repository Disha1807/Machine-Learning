{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are done creating hundreds of thousands of features, it’s time for \n",
    "selecting a few of them."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Having too many features pose a problem well known as the curse of dimensionality."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The simplest form of selecting features would be to remove features with very \n",
    "low variance. \n",
    "\n",
    "If the features have a very low variance (i.e. very close to 0), they \n",
    "are close to being constant and thus, do not add any value to any model at all."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Scikit-learn has an implementation for `VarianceThreshold` that does precisely this.\n",
    "\n",
    "     - transformed data will have all columns with variance less than 0.1 removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "float() argument must be a string or a number, not 'ellipsis'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\lenovo\\Desktop\\Data Science\\machine learning\\notebooks\\feature_selection.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lenovo/Desktop/Data%20Science/machine%20learning/notebooks/feature_selection.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m data \u001b[39m=\u001b[39m \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lenovo/Desktop/Data%20Science/machine%20learning/notebooks/feature_selection.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m var_thresh \u001b[39m=\u001b[39m VarianceThreshold(threshold\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/lenovo/Desktop/Data%20Science/machine%20learning/notebooks/feature_selection.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m transformed_data \u001b[39m=\u001b[39m var_thresh\u001b[39m.\u001b[39;49mfit_transform(data)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:878\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    874\u001b[0m \u001b[39m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[0;32m    875\u001b[0m \u001b[39m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[0;32m    876\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    877\u001b[0m     \u001b[39m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n\u001b[0;32m    879\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    880\u001b[0m     \u001b[39m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m    881\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\feature_selection\\_variance_threshold.py:98\u001b[0m, in \u001b[0;36mVarianceThreshold.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[39m\"\"\"Learn empirical variances from X.\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \n\u001b[0;32m     82\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[39m    Returns the instance itself.\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m---> 98\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m     99\u001b[0m     X,\n\u001b[0;32m    100\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcsc\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m    101\u001b[0m     dtype\u001b[39m=\u001b[39;49mnp\u001b[39m.\u001b[39;49mfloat64,\n\u001b[0;32m    102\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    103\u001b[0m )\n\u001b[0;32m    105\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(X, \u001b[39m\"\u001b[39m\u001b[39mtoarray\u001b[39m\u001b[39m\"\u001b[39m):  \u001b[39m# sparse matrix\u001b[39;00m\n\u001b[0;32m    106\u001b[0m     _, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvariances_ \u001b[39m=\u001b[39m mean_variance_axis(X, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:565\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    563\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mValidation should be done on X, y or both.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    564\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 565\u001b[0m     X \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[0;32m    566\u001b[0m     out \u001b[39m=\u001b[39m X\n\u001b[0;32m    567\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\utils\\validation.py:879\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    877\u001b[0m         array \u001b[39m=\u001b[39m xp\u001b[39m.\u001b[39mastype(array, dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    878\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 879\u001b[0m         array \u001b[39m=\u001b[39m _asarray_with_order(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype, xp\u001b[39m=\u001b[39;49mxp)\n\u001b[0;32m    880\u001b[0m \u001b[39mexcept\u001b[39;00m ComplexWarning \u001b[39mas\u001b[39;00m complex_warning:\n\u001b[0;32m    881\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    882\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mComplex data not supported\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[0;32m    883\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\utils\\_array_api.py:185\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    182\u001b[0m     xp, _ \u001b[39m=\u001b[39m get_namespace(array)\n\u001b[0;32m    183\u001b[0m \u001b[39mif\u001b[39;00m xp\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39min\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mnumpy\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mnumpy.array_api\u001b[39m\u001b[39m\"\u001b[39m}:\n\u001b[0;32m    184\u001b[0m     \u001b[39m# Use NumPy API to support order\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     array \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39;49masarray(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[0;32m    186\u001b[0m     \u001b[39mreturn\u001b[39;00m xp\u001b[39m.\u001b[39masarray(array, copy\u001b[39m=\u001b[39mcopy)\n\u001b[0;32m    187\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'ellipsis'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "data = ...\n",
    "var_thresh = VarianceThreshold(threshold=0.1)\n",
    "transformed_data = var_thresh.fit_transform(data)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- We can also remove features which have a high correlation.\n",
    "\n",
    "- you can use the **Pearson correlation**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# fetch a regression dataset\n",
    "data = fetch_california_housing()\n",
    "X = data[\"data\"]\n",
    "col_names = data[\"feature_names\"]\n",
    "y = data[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(X, columns=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# introduce a highly correlated column\n",
    "df.loc[:, \"MedInc_Sqrt\"] = df.MedInc.apply(np.sqrt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>MedInc_Sqrt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MedInc</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.119034</td>\n",
       "      <td>0.326895</td>\n",
       "      <td>-0.062040</td>\n",
       "      <td>0.004834</td>\n",
       "      <td>0.018766</td>\n",
       "      <td>-0.079809</td>\n",
       "      <td>-0.015176</td>\n",
       "      <td>0.984329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HouseAge</th>\n",
       "      <td>-0.119034</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.153277</td>\n",
       "      <td>-0.077747</td>\n",
       "      <td>-0.296244</td>\n",
       "      <td>0.013191</td>\n",
       "      <td>0.011173</td>\n",
       "      <td>-0.108197</td>\n",
       "      <td>-0.132797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AveRooms</th>\n",
       "      <td>0.326895</td>\n",
       "      <td>-0.153277</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.847621</td>\n",
       "      <td>-0.072213</td>\n",
       "      <td>-0.004852</td>\n",
       "      <td>0.106389</td>\n",
       "      <td>-0.027540</td>\n",
       "      <td>0.326688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AveBedrms</th>\n",
       "      <td>-0.062040</td>\n",
       "      <td>-0.077747</td>\n",
       "      <td>0.847621</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.066197</td>\n",
       "      <td>-0.006181</td>\n",
       "      <td>0.069721</td>\n",
       "      <td>0.013344</td>\n",
       "      <td>-0.066910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Population</th>\n",
       "      <td>0.004834</td>\n",
       "      <td>-0.296244</td>\n",
       "      <td>-0.072213</td>\n",
       "      <td>-0.066197</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.069863</td>\n",
       "      <td>-0.108785</td>\n",
       "      <td>0.099773</td>\n",
       "      <td>0.018415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AveOccup</th>\n",
       "      <td>0.018766</td>\n",
       "      <td>0.013191</td>\n",
       "      <td>-0.004852</td>\n",
       "      <td>-0.006181</td>\n",
       "      <td>0.069863</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002366</td>\n",
       "      <td>0.002476</td>\n",
       "      <td>0.015266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Latitude</th>\n",
       "      <td>-0.079809</td>\n",
       "      <td>0.011173</td>\n",
       "      <td>0.106389</td>\n",
       "      <td>0.069721</td>\n",
       "      <td>-0.108785</td>\n",
       "      <td>0.002366</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.924664</td>\n",
       "      <td>-0.084303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Longitude</th>\n",
       "      <td>-0.015176</td>\n",
       "      <td>-0.108197</td>\n",
       "      <td>-0.027540</td>\n",
       "      <td>0.013344</td>\n",
       "      <td>0.099773</td>\n",
       "      <td>0.002476</td>\n",
       "      <td>-0.924664</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.015569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MedInc_Sqrt</th>\n",
       "      <td>0.984329</td>\n",
       "      <td>-0.132797</td>\n",
       "      <td>0.326688</td>\n",
       "      <td>-0.066910</td>\n",
       "      <td>0.018415</td>\n",
       "      <td>0.015266</td>\n",
       "      <td>-0.084303</td>\n",
       "      <td>-0.015569</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  \\\n",
       "MedInc       1.000000 -0.119034  0.326895  -0.062040    0.004834  0.018766   \n",
       "HouseAge    -0.119034  1.000000 -0.153277  -0.077747   -0.296244  0.013191   \n",
       "AveRooms     0.326895 -0.153277  1.000000   0.847621   -0.072213 -0.004852   \n",
       "AveBedrms   -0.062040 -0.077747  0.847621   1.000000   -0.066197 -0.006181   \n",
       "Population   0.004834 -0.296244 -0.072213  -0.066197    1.000000  0.069863   \n",
       "AveOccup     0.018766  0.013191 -0.004852  -0.006181    0.069863  1.000000   \n",
       "Latitude    -0.079809  0.011173  0.106389   0.069721   -0.108785  0.002366   \n",
       "Longitude   -0.015176 -0.108197 -0.027540   0.013344    0.099773  0.002476   \n",
       "MedInc_Sqrt  0.984329 -0.132797  0.326688  -0.066910    0.018415  0.015266   \n",
       "\n",
       "             Latitude  Longitude  MedInc_Sqrt  \n",
       "MedInc      -0.079809  -0.015176     0.984329  \n",
       "HouseAge     0.011173  -0.108197    -0.132797  \n",
       "AveRooms     0.106389  -0.027540     0.326688  \n",
       "AveBedrms    0.069721   0.013344    -0.066910  \n",
       "Population  -0.108785   0.099773     0.018415  \n",
       "AveOccup     0.002366   0.002476     0.015266  \n",
       "Latitude     1.000000  -0.924664    -0.084303  \n",
       "Longitude   -0.924664   1.000000    -0.015569  \n",
       "MedInc_Sqrt -0.084303  -0.015569     1.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get correlation matrix (pearson)\n",
    "df.corr()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the feature MedInc_Sqrt has a very high correlation with MedInc. We \n",
    "can thus remove one of them. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### univariate ways of feature selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Univariate feature selection` is nothing but a scoring of each feature against a given target. \n",
    "\n",
    "- Mutual information\n",
    "- ANOVA F-test  \n",
    "- chi<sup>2</sup>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways of using these in scikit-learn. \n",
    "- `SelectKBest`: It keeps the top-k scoring features.\n",
    "- `SelectPercentile`: It keeps the top features which are in a percentage \n",
    "specified by the user."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- you can use `chi`<sup>2</sup> only for data which is non-negative in nature.\n",
    "\n",
    "- This is a particularly useful feature selection technique in natural language \n",
    "processing when we have a bag of words or tf-idf based features. \n",
    "\n",
    "- It’s best to create a wrapper for univariate feature selection that you can use for almost any new \n",
    "problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import SelectPercentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnivariateFeatureSelction:\n",
    " def __init__(self, n_features, problem_type, scoring):\n",
    "    \"\"\"\n",
    "    Custom univariate feature selection wrapper on\n",
    "    different univariate feature selection models from\n",
    "    scikit-learn.\n",
    "    :param n_features: SelectPercentile if float else SelectKBest\n",
    "    :param problem_type: classification or regression\n",
    "    :param scoring: scoring function, string\n",
    "    \"\"\"\n",
    "    # for a given problem type, there are only\n",
    "    # a few valid scoring methods\n",
    "    # you can extend this with your own custom\n",
    "    # methods if you wish\n",
    "    if problem_type == \"classification\":\n",
    "        valid_scoring = {\n",
    "        \"f_classif\": f_classif,\n",
    "        \"chi2\": chi2,\n",
    "        \"mutual_info_classif\": mutual_info_classif\n",
    "        }\n",
    "    else:\n",
    "        valid_scoring = {\n",
    "            \"f_regression\": f_regression,\n",
    "            \"mutual_info_regression\": mutual_info_regression\n",
    "            }\n",
    " \n",
    "    # raise exception if we do not have a valid scoring method\n",
    "    if scoring not in valid_scoring:\n",
    "        raise Exception(\"Invalid scoring function\")\n",
    "    \n",
    "    # if n_features is int, we use selectkbest\n",
    "    # if n_features is float, we use selectpercentile\n",
    "    # please note that it is int in both cases in sklearn\n",
    "    if isinstance(n_features, int):\n",
    "        self.selection = SelectKBest(\n",
    "        valid_scoring[scoring],\n",
    "        k=n_features\n",
    "        )\n",
    "\n",
    "    elif isinstance(n_features, float):\n",
    "        self.selection = SelectPercentile(\n",
    "        valid_scoring[scoring],\n",
    "        percentile=int(n_features * 100)\n",
    "    )\n",
    "    else:\n",
    "        raise Exception(\"Invalid type of feature\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same fit function\n",
    "def fit(self, X, y):\n",
    " return self.selection.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same transform function\n",
    "def transform(self, X):\n",
    " return self.selection.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same fit_transform function\n",
    "def fit_transform(self, X, y):\n",
    " return self.selection.fit_transform(X, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this class is pretty simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ufs = UnivariateFeatureSelction(\n",
    " n_features=0.1, \n",
    " problem_type=\"regression\", \n",
    " scoring=\"f_regression\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Univariate feature selection may not always perform \n",
    "well. Most of the time, people prefer doing feature selection using a machine \n",
    "learning model. Let’s see how that is done."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Most of the time, people prefer doing feature selection using a machine \n",
    "learning model. Let’s see how that is done.\n",
    "\n",
    "- The simplest form of feature selection that uses a model for selection is known as \n",
    "`greedy feature selection`. \n",
    "\n",
    "- In greedy feature selection, \n",
    "    - the first step is to choose a model. \n",
    "    - The second step is to select a loss/scoring function. \n",
    "    - And the third and final step is to iteratively evaluate each feature and add it to the list of “good” features if it improves loss/score.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Disadvantages`\n",
    "\n",
    "- The computational cost associated with this kind of method is very high.\n",
    "- It will also take a lot of time for this kind of feature selection to finish.\n",
    "- And if you do not use this feature selection properly, then you might even end up overfitting the model. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Impelementation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_score(X, y):\n",
    "    \"\"\"\n",
    "    This function evaluates model on data and returns\n",
    "    Area Under ROC Curve (AUC)\n",
    "    NOTE: We fit the data and calculate AUC on same data.\n",
    "    WE ARE OVERFITTING HERE. \n",
    "    But this is also a way to achieve greedy selection.\n",
    "    k-fold will take k times longer.\n",
    "    If you want to implement it in really correct way,\n",
    "    calculate OOF AUC and return mean AUC over k folds.\n",
    "    This requires only a few lines of change and has been \n",
    "    shown a few times in this book.\n",
    "    :param X: training data\n",
    "    :param y: targets\n",
    "    :return: overfitted area under the roc curve\n",
    "    \"\"\"\n",
    "    # fit the logistic regression model,\n",
    "    # and calculate AUC on same data\n",
    "    # again: BEWARE\n",
    "    # you can choose any model that suits your data\n",
    "\n",
    "    model = linear_model.LogisticRegression()\n",
    "    model.fit(X, y)\n",
    "    predictions = model.predict_proba(X)[:, 1]\n",
    "    auc = metrics.roc_auc_score(y, predictions)\n",
    "    return auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _feature_selection(X, y):\n",
    "    \"\"\"\n",
    "    This function does the actual greedy selection\n",
    "    :param X: data, numpy array\n",
    "    :param y: targets, numpy array\n",
    "    :return: (best scores, best features)\n",
    "    \"\"\"\n",
    "    # initialize good features list \n",
    "    # and best scores to keep track of both\n",
    "    good_features = []\n",
    "    best_scores = []\n",
    "    \n",
    "    # calculate the number of features\n",
    "    num_features = X.shape[1]\n",
    "    \n",
    "    # infinite loop\n",
    "    while True:\n",
    "        # initialize best feature and score of this loop\n",
    "        this_feature = None\n",
    "        best_score = 0\n",
    "        # loop over all features\n",
    "        for feature in range(num_features):\n",
    "            # if feature is already in good features,\n",
    "            # skip this for loop\n",
    "            if feature in good_features:\n",
    "                continue\n",
    "            # selected features are all good features till now\n",
    "            # and current feature\n",
    "            selected_features = good_features + [feature]\n",
    "            # remove all other features from data\n",
    "            xtrain = X[:, selected_features]\n",
    "            # calculate the score, in our case, AUC\n",
    "            score = evaluate_score(xtrain, y)\n",
    "            # if score is greater than the best score\n",
    "            # of this loop, change best score and best feature\n",
    "            if score > best_score:\n",
    "                this_feature = feature\n",
    "                best_score = score\n",
    "            # if we have selected a feature, add it\n",
    "\n",
    "            # to the good feature list and update best scores list\n",
    "            if this_feature != None:\n",
    "                good_features.append(this_feature)\n",
    "                best_scores.append(best_score)\n",
    "            # if we didnt improve during the previous round,\n",
    "            # exit the while loop\n",
    "            if len(best_scores) > 2:\n",
    "                if best_scores[-1] < best_scores[-2]:\n",
    "                    break\n",
    "            # return best scores and good features\n",
    "            # why do we remove the last data point?\n",
    "            return best_scores[:-1], good_features[:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call(X, y):\n",
    " \"\"\"\n",
    " Call function will call the class on a set of arguments\n",
    " \"\"\"\n",
    " # select features, return scores and selected indices\n",
    " scores, features = _feature_selection(X, y)\n",
    " # transform data with selected features\n",
    " return X[:, features], scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate binary classification data\n",
    "X, y = make_classification(n_samples=1000, n_features=100)\n",
    "\n",
    "# transform data by greedy feature selection\n",
    "X_transformed, scores = call(X, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another greedy approach is known as `recursive feature elimination (RFE)`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RFE, we start with all features and keep removing one feature in every iteration that provides the least value to a given model.\n",
    "\n",
    "- if we use models like linear support vector machine (SVM) or logistic regression, we get a coefficient for each feature which decides the importance of the features.\n",
    "\n",
    "- In case of any tree-based models, we get feature importance in place of coefficients.\n",
    "\n",
    "-  In each iteration, we remove the feature which has the feature importance or the feature which has a coefficient close to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import fetch_california_housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch a regression dataset\n",
    "data = fetch_california_housing()\n",
    "X = data[\"data\"]\n",
    "col_names = data[\"feature_names\"]\n",
    "y = data[\"target\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model\n",
    "model = LinearRegression()\n",
    "# initialize RFE\n",
    "rfe = RFE(\n",
    " estimator=model,\n",
    " n_features_to_select=3\n",
    ")\n",
    "# fit RFE\n",
    "rfe.fit(X, y)\n",
    "# get the transformed data with\n",
    "# selected columns\n",
    "X_transformed = rfe.transform(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`feature coefficients or the importance of features`\n",
    "    If you use coefficients, you can select a threshold, and if the coefficient is above that threshold, you can keep the feature \n",
    "else eliminate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch a regression dataset\n",
    "# in diabetes data we predict diabetes progression\n",
    "# after one year based on some features\n",
    "data = load_diabetes()\n",
    "X = data[\"data\"]\n",
    "col_names = data[\"feature_names\"]\n",
    "y = data[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model\n",
    "model = RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor()"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(X, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature importance from random forest (or any model) can be plotted as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "importances = model.feature_importances_\n",
    "idxs = np.argsort(importances)\n",
    "plt.title('Feature Importances')\n",
    "plt.barh(range(len(idxs)), importances[idxs], align='center')\n",
    "plt.yticks(range(len(idxs)), [col_names[i] for i in idxs])\n",
    "plt.xlabel('Random Forest Feature Importance')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn also offers `SelectFromModel` class that helps you choose features directly from a given model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also specify the threshold for coefficients or feature importance if you want and the \n",
    "maximum number of features you want to select."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch a regression dataset\n",
    "# in diabetes data we predict diabetes progression\n",
    "# after one year based on some features\n",
    "data = load_diabetes()\n",
    "X = data[\"data\"]\n",
    "col_names = data[\"feature_names\"]\n",
    "y = data[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model\n",
    "model = RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select from the model\n",
    "sfm = SelectFromModel(estimator=model)\n",
    "X_transformed = sfm.fit_transform(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see which features were selected\n",
    "support = sfm.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bmi', 's5']\n"
     ]
    }
   ],
   "source": [
    "# get feature names\n",
    "print([\n",
    " x for x, y in zip(col_names, support) if y == True\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  One more thing that we are missing here is feature selection using models that have L1 (Lasso) penalization. \n",
    "- When we have L1 penalization for regularization, most coefficients will be 0 (or close to 0), and we select the features with non-zero coefficients. \n",
    "- You can do it by just replacing random forest in the snippet of selection from a model with a model that supports \n",
    "L1 penalty, e.g. lasso regression.\n",
    "\n",
    "- All tree-based models provide feature importance so all the model-based snippets shown can be used for XGBoost, \n",
    "LightGBM or CatBoost."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Select features on  training data and validate the model on validation data for proper selection of features without overfitting the model.`\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
